{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3a72667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3bbce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the pipe-delimited text file\n",
    "df = pd.read_csv('../data/MachineLearningRating_v3.txt', sep='|')\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv('../data/data.csv', index=False)\n",
    "\n",
    "print(\"Conversion completed: 'data.csv' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca5f2989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mijuu\\AppData\\Local\\Temp\\ipykernel_27960\\538447513.py:1: DtypeWarning: Columns (32,37) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"../data/data.csv\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 52\n",
      "\n",
      "Column data types:\n",
      "UnderwrittenCoverID           int64\n",
      "PolicyID                      int64\n",
      "TransactionMonth             object\n",
      "IsVATRegistered                bool\n",
      "Citizenship                  object\n",
      "LegalType                    object\n",
      "Title                        object\n",
      "Language                     object\n",
      "Bank                         object\n",
      "AccountType                  object\n",
      "MaritalStatus                object\n",
      "Gender                       object\n",
      "Country                      object\n",
      "Province                     object\n",
      "PostalCode                    int64\n",
      "MainCrestaZone               object\n",
      "SubCrestaZone                object\n",
      "ItemType                     object\n",
      "mmcode                      float64\n",
      "VehicleType                  object\n",
      "RegistrationYear              int64\n",
      "make                         object\n",
      "Model                        object\n",
      "Cylinders                   float64\n",
      "cubiccapacity               float64\n",
      "kilowatts                   float64\n",
      "bodytype                     object\n",
      "NumberOfDoors               float64\n",
      "VehicleIntroDate             object\n",
      "CustomValueEstimate         float64\n",
      "AlarmImmobiliser             object\n",
      "TrackingDevice               object\n",
      "CapitalOutstanding           object\n",
      "NewVehicle                   object\n",
      "WrittenOff                   object\n",
      "Rebuilt                      object\n",
      "Converted                    object\n",
      "CrossBorder                  object\n",
      "NumberOfVehiclesInFleet     float64\n",
      "SumInsured                  float64\n",
      "TermFrequency                object\n",
      "CalculatedPremiumPerTerm    float64\n",
      "ExcessSelected               object\n",
      "CoverCategory                object\n",
      "CoverType                    object\n",
      "CoverGroup                   object\n",
      "Section                      object\n",
      "Product                      object\n",
      "StatutoryClass               object\n",
      "StatutoryRiskType            object\n",
      "TotalPremium                float64\n",
      "TotalClaims                 float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/data.csv\")\n",
    "# Get number of columns\n",
    "num_columns = df.shape[1]\n",
    "print(f\"Number of columns: {num_columns}\")\n",
    "\n",
    "# Get column data types\n",
    "print(\"\\nColumn data types:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119b4bf",
   "metadata": {},
   "source": [
    "Data Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ce2ce62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaN in each column (before imputation):\n",
      "TotalPremium                      0\n",
      "TotalClaims                       0\n",
      "CalculatedPremiumPerTerm          0\n",
      "SumInsured                        0\n",
      "CustomValueEstimate          779642\n",
      "NumberOfVehiclesInFleet     1000098\n",
      "CapitalOutstanding              322\n",
      "dtype: int64\n",
      "\n",
      "Number of rows with NaN in each column (after imputing CapitalOutstanding):\n",
      "TotalPremium                      0\n",
      "TotalClaims                       0\n",
      "CalculatedPremiumPerTerm          0\n",
      "SumInsured                        0\n",
      "CustomValueEstimate          779642\n",
      "NumberOfVehiclesInFleet     1000098\n",
      "CapitalOutstanding                0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mijuu\\AppData\\Local\\Temp\\ipykernel_27960\\374191108.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"CapitalOutstanding\"].fillna(median_value, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rows with at least one NaN in numeric columns: 1000098\n",
      "\n",
      "Total columns still containing NaNs in the entire DataFrame:\n",
      "Bank                        145961\n",
      "AccountType                  40232\n",
      "MaritalStatus                 8259\n",
      "Gender                        9536\n",
      "mmcode                         552\n",
      "VehicleType                    552\n",
      "make                           552\n",
      "Model                          552\n",
      "Cylinders                      552\n",
      "cubiccapacity                  552\n",
      "kilowatts                      552\n",
      "bodytype                       552\n",
      "NumberOfDoors                  552\n",
      "VehicleIntroDate               552\n",
      "CustomValueEstimate         779642\n",
      "NewVehicle                  153295\n",
      "WrittenOff                  641901\n",
      "Rebuilt                     641901\n",
      "Converted                   641901\n",
      "CrossBorder                 999400\n",
      "NumberOfVehiclesInFleet    1000098\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define numerical columns of interest\n",
    "numeric_cols = [\n",
    "    \"TotalPremium\",\n",
    "    \"TotalClaims\",\n",
    "    \"CalculatedPremiumPerTerm\",\n",
    "    \"SumInsured\",\n",
    "    \"CustomValueEstimate\",\n",
    "    \"NumberOfVehiclesInFleet\",\n",
    "    \"CapitalOutstanding\"\n",
    "]\n",
    "\n",
    "# Convert these columns to numeric, coercing errors to NaN\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Initial NaN count (before imputation)\n",
    "nan_counts_before = df[numeric_cols].isna().sum()\n",
    "print(\"Number of rows with NaN in each column (before imputation):\")\n",
    "print(nan_counts_before)\n",
    "\n",
    "# Impute CapitalOutstanding with median\n",
    "median_value = df[\"CapitalOutstanding\"].median()\n",
    "df[\"CapitalOutstanding\"].fillna(median_value, inplace=True)\n",
    "\n",
    "# Recalculate NaN count (after imputation)\n",
    "nan_counts_after = df[numeric_cols].isna().sum()\n",
    "print(\"\\nNumber of rows with NaN in each column (after imputing CapitalOutstanding):\")\n",
    "print(nan_counts_after)\n",
    "\n",
    "# Count rows with at least one NaN in numeric columns\n",
    "num_nan_rows = df[df[numeric_cols].isna().any(axis=1)].shape[0]\n",
    "print(f\"\\nNumber of rows with at least one NaN in numeric columns: {num_nan_rows}\")\n",
    "\n",
    "# Optional: Check how many columns in the entire DataFrame still have NaNs\n",
    "cols_with_nan = df.isna().sum()\n",
    "print(\"\\nTotal columns still containing NaNs in the entire DataFrame:\")\n",
    "print(cols_with_nan[cols_with_nan > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17927da",
   "metadata": {},
   "source": [
    "handle missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61b42402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Columns with Extremely High Missingness & Low Predictive Value\n",
    "df.drop(columns=[\n",
    "    \"CustomValueEstimate\", \"NumberOfVehiclesInFleet\",\n",
    "    \"WrittenOff\", \"Rebuilt\", \"Converted\", \"CrossBorder\"\n",
    "], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "345787e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mijuu\\AppData\\Local\\Temp\\ipykernel_27960\\1606557252.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(\"Unknown\", inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#Categorical Variables — Impute with \"Unknown\" or Mode\n",
    "categorical_cols = [\"Bank\", \"AccountType\", \"MaritalStatus\", \"Gender\", \n",
    "                    \"VehicleType\", \"make\", \"Model\", \"bodytype\", \"NewVehicle\"]\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df[col].fillna(\"Unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28b5d37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mijuu\\AppData\\Local\\Temp\\ipykernel_27960\\2224009588.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\mijuu\\AppData\\Local\\Temp\\ipykernel_27960\\2224009588.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\mijuu\\AppData\\Local\\Temp\\ipykernel_27960\\2224009588.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\mijuu\\AppData\\Local\\Temp\\ipykernel_27960\\2224009588.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(median_val, inplace=True)\n",
      "C:\\Users\\mijuu\\AppData\\Local\\Temp\\ipykernel_27960\\2224009588.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(median_val, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#Numerical Variables — Impute with Median\n",
    "num_cols = [\"mmcode\", \"Cylinders\", \"cubiccapacity\", \"kilowatts\", \"NumberOfDoors\"]\n",
    "\n",
    "for col in num_cols:\n",
    "    median_val = df[col].median()\n",
    "    df[col].fillna(median_val, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99b39ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of columns: 46\n"
     ]
    }
   ],
   "source": [
    "# Get number of columns\n",
    "num_columns = df.shape[1]\n",
    "print(f\"Number of columns: {num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cbd6a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total columns still containing NaNs in the entire DataFrame:\n",
      "VehicleIntroDate    552\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Optional: Check how many columns in the entire DataFrame still have NaNs\n",
    "cols_with_nan_after_handling = df.isna().sum()\n",
    "print(\"\\nTotal columns still containing NaNs in the entire DataFrame:\")\n",
    "print(cols_with_nan_after_handling[cols_with_nan_after_handling > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "308e978d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/cleaned_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343489fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute basic descriptive statistics\n",
    "descriptive_stats = df[numeric_cols].describe().T  # Transpose for better readability\n",
    "\n",
    "# Add variance and coefficient of variation (CV = std / mean)\n",
    "descriptive_stats[\"variance\"] = df[numeric_cols].var()\n",
    "descriptive_stats[\"cv\"] = descriptive_stats[\"std\"] / descriptive_stats[\"mean\"]\n",
    "\n",
    "# Display results\n",
    "print(descriptive_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81b34d4",
   "metadata": {},
   "source": [
    "Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d93ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data types of each column\n",
    "print(\"Column Data Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Summary of column types\n",
    "print(\"\\nSummary of Column Types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "# Identify columns by inferred type\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "bool_cols = df.select_dtypes(include=['bool']).columns.tolist()\n",
    "\n",
    "# Display lists\n",
    "print(f\"\\nCategorical Columns ({len(categorical_cols)}):\\n{categorical_cols}\")\n",
    "print(f\"\\nNumerical Columns ({len(numerical_cols)}):\\n{numerical_cols}\")\n",
    "print(f\"\\nBoolean Columns ({len(bool_cols)}):\\n{bool_cols}\")\n",
    "\n",
    "# Check for possible datetime fields\n",
    "print(\"\\nChecking object columns for potential date fields:\")\n",
    "for col in categorical_cols:\n",
    "    try:\n",
    "        parsed = pd.to_datetime(df[col], errors='raise')\n",
    "        print(f\"✅ '{col}' can be converted to datetime.\")\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7043b331",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d005f9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of plots per row\n",
    "plots_per_row = 3\n",
    "num_cols = len(numerical_cols)\n",
    "num_rows = math.ceil(num_cols / plots_per_row)\n",
    "\n",
    "# Set figure size based on number of rows/columns\n",
    "plt.figure(figsize=(plots_per_row * 6, num_rows * 4))\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Plot histograms in a grid layout\n",
    "for idx, col in enumerate(numerical_cols, start=1):\n",
    "    plt.subplot(num_rows, plots_per_row, idx)\n",
    "    sns.histplot(df[col].dropna(), kde=True, bins=30, color='skyblue')\n",
    "    plt.title(f\"{col}\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel(\"\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6713861",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style for plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 6)\n",
    "\n",
    "# --- 2. Bar Charts for Categorical Variables ---\n",
    "for col in categorical_cols:\n",
    "    plt.figure()\n",
    "    top_values = df[col].value_counts().nlargest(10)\n",
    "    sns.barplot(x=top_values.index, y=top_values.values)\n",
    "    plt.title(f\"Top 10 Categories in '{col}'\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# --- 3. Bar Charts for Boolean Variables ---\n",
    "for col in bool_cols:\n",
    "    plt.figure()\n",
    "    sns.countplot(x=df[col])\n",
    "    plt.title(f\"Distribution of Boolean Feature: '{col}'\")\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39a3cf5",
   "metadata": {},
   "source": [
    "Bivariate / Multivariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdfb3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert 'TransactionMonth' to datetime\n",
    "df['TransactionMonth'] = pd.to_datetime(df['TransactionMonth'], errors='coerce')\n",
    "\n",
    "# 2. Group by PostalCode and Month, then aggregate\n",
    "grouped = df.groupby(['PostalCode', pd.Grouper(key='TransactionMonth', freq='M')])[\n",
    "    ['TotalPremium', 'TotalClaims']\n",
    "].sum().reset_index()\n",
    "\n",
    "# 3. Scatter Plot: TotalPremium vs TotalClaims\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=grouped, x='TotalPremium', y='TotalClaims', hue='PostalCode', palette='tab20', legend=False)\n",
    "plt.title(\"TotalPremium vs TotalClaims by PostalCode per Month\")\n",
    "plt.xlabel(\"Total Premium (monthly, per Zip)\")\n",
    "plt.ylabel(\"Total Claims (monthly, per Zip)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Correlation Matrix: Numerical aggregates per PostalCode\n",
    "postal_corr = grouped.groupby(\"PostalCode\")[['TotalPremium', 'TotalClaims']].sum().corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(postal_corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix: TotalPremium & TotalClaims (Aggregated by ZipCode)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcbeabd",
   "metadata": {},
   "source": [
    "Data Comparison Over Geography"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba6ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
    "\n",
    "# --- 1. Compare CoverType distribution by Province ---\n",
    "cover_geo = df.groupby(['Province', 'CoverType']).size().unstack().fillna(0)\n",
    "\n",
    "cover_geo.plot(kind='bar', stacked=True)\n",
    "plt.title(\"CoverType Distribution by Province\")\n",
    "plt.ylabel(\"Number of Policies\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 2. Compare average TotalPremium by Province ---\n",
    "premium_geo = df.groupby('Province')['TotalPremium'].mean().sort_values()\n",
    "\n",
    "premium_geo.plot(kind='bar', color='teal')\n",
    "plt.title(\"Average Total Premium by Province\")\n",
    "plt.ylabel(\"Average Premium\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- 3. Compare most popular Auto Make by Province (Top 5 Makes only) ---\n",
    "top_makes = df['make'].value_counts().nlargest(5).index.tolist()\n",
    "df_top_makes = df[df['make'].isin(top_makes)]\n",
    "\n",
    "make_geo = df_top_makes.groupby(['Province', 'make']).size().unstack().fillna(0)\n",
    "\n",
    "make_geo.plot(kind='bar', stacked=True)\n",
    "plt.title(\"Top 5 Auto Makes Distribution by Province\")\n",
    "plt.ylabel(\"Number of Vehicles\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
